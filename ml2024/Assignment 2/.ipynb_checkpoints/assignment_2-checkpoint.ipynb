{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "675b8f31",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5371f06b425db83a39c241952527eaeb",
     "grade": false,
     "grade_id": "cell-d8c0fc514422446a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "## Lecture 3 & 4 - Classification & Feature learning, visualization\n",
    "\n",
    "### Learning Goals\n",
    "After successfully completing this assignment, you should be able to:\n",
    "- differentiate between classification and regression tasks\n",
    "- learn a hypothesis for binary classification by using logistic regression and evaluate it\n",
    "- evaluate a hypothesis for classification problems by using different metrics\n",
    "- learn a hypothesis for multi-class classification problems (more than two different label values) and evaluate it\n",
    "- understand why one might want to use a lower number of features\n",
    "- understand PCA on an intuitive level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167707e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2fb5ae009fa6c2cc4c871e146e8d4172",
     "grade": false,
     "grade_id": "cell-4a8fc96b052e9e8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Table of Contents\n",
    "1. [Classification](#Classification)\n",
    "2. [Visualization](#Visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef1b065",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2542f53fb411d2eb748bc4fc63ed5363",
     "grade": false,
     "grade_id": "cell-b053931adc1b9349",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## General Instructions\n",
    "- Jupyter notebook is supposed to be run cell by cell in order, please do not skip any code cell, this will cause some errors. Also running cells back and forth sometimes might also incur errors. If you feel you lost your track, you can click \"Kernel->Restart\" from the menu to restart the process.\n",
    "- Before submitting your assignment, ensure that it does not contain trivial errors by pressing the \"validate\" button at the top.\n",
    "- Your implementations are supposed to be added to the places where it reads \"YOUR CODE HERE\". Please also remove the \"raise NotImplementedError()\" line before submitting.\n",
    "- Please DO NOT change the metadata of any cell, cells for demo and instructions are not editable.\n",
    "- Please DO NOT change the order of solution cell and test cell, you will lose points if the order is changed.\n",
    "- You can copy lines of code from cells that are not editable, but please DO NOT copy and paste them as cells, this may incur validation error. \n",
    "- You can add extra cells or code to help double-check your solution, but please make sure that variables required by tasks are not overwritten, or just delete those extra cells before submitting.\n",
    "- Please DO NOT change file names in you submission, renamed files can not be recognized by the grading system.\n",
    "- Reading the documentation of Python libraries is always a good practice, all the Python libraries (Numpy, Pandas, Sklearn,etc.) we utilized in this course provide very well organized documentation for each method/class/function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313f5123",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T06:52:16.597034Z",
     "start_time": "2022-01-25T06:52:13.757048Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d0f2600c33fd97e57381ba7f6b1f661",
     "grade": false,
     "grade_id": "cell-ba12c5c47b74176a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False  # enable code auto-completion\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  #data visualization library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix  # evaluation metrics\n",
    "\n",
    "from sklearn.datasets import fetch_openml \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f91470",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86512663e356aefa2bc9d542fc8247d0",
     "grade": false,
     "grade_id": "cell-6b6cb1005aa67d18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3007184",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "620801678915e3a2dd906ebf5a2912c8",
     "grade": false,
     "grade_id": "cell-1aa5d80bc5076d91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Dataset \n",
    "We are going to use weather recordings from the [Finnish Meteorological Institute](https://en.ilmatieteenlaitos.fi/). For your convenience we have already downloaded and stored these recordings in the csv file `FMIData.csv`.  The code snippet below reads in the weather recordings from this file and store them in a Pandas `DataFrame` with the name `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe485bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T06:52:16.634919Z",
     "start_time": "2022-01-25T06:52:16.599288Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5e62d7c57d88b9512d95ddc94d94d76",
     "grade": false,
     "grade_id": "cell-60a7f3a9ad13dae2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in the data stored in the file 'FMIData_Assignment.csv'\n",
    "# Clean the dataframe\n",
    "\n",
    "df = pd.read_csv('FMIData.csv')\n",
    "df.drop(columns=['Time zone','Precipitation amount (mm)','Snow depth (cm)','Air temperature (degC)',\\\n",
    "                 'Ground minimum temperature (degC)'],inplace=True)  # drop unrelevant columns\n",
    "\n",
    "df.columns =['year','m','d','time','max temperature','min temperature'] # rename columns \n",
    "\n",
    "# Print the first 5 rows of the DataFrame 'df'\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f7f94",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3920c5835627a3def2c917e57cd89da0",
     "grade": false,
     "grade_id": "cell-c8a753eada83fc7c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Each row of dataframe `df` contains a weather recording along with a time-stamp. We will use only weather recording with time-stamp `00:00`. For the $i$-th day, we use the value in the column \"min temperature\" as the feature $x^{(i)}$. The label $y^{(i)}$ of the $i$-th day (datapoint) is determined by the value $\\text{maxtmp}^{(i)}$ in the column \"max temperature\". In particular, we define the label $y^{(i)}$ of the $i$-th day to be equal to 1 if the corresponding weather recording has a positive entry in  the column \"max temperature\", otherwise we define the label to be $y^{(i)}=0$. More formally, $y^{(i)}=1$ if $\\text{maxtmp}^{(i)} > 0$ and $y^{(i)}=0$ if $\\text{maxtmp}^{(i)} \\leq{0}$. This can be easily realized by using the Pandas method [pd.cut()](https://pandas.pydata.org/docs/reference/api/pandas.cut.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a20d9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T06:52:16.675912Z",
     "start_time": "2022-01-25T06:52:16.641138Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92be8aedc09a0f78444b62cc235eec08",
     "grade": false,
     "grade_id": "cell-c99db19d6650a759",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select only weather recordings whose property 'time' is equal to `00:00`\n",
    "FMIRawData = df[df['time'] == '00:00']\n",
    "\n",
    "minvalue = FMIRawData['max temperature'].min() # minimum value of the column 'max temperature'\n",
    "maxvalue = FMIRawData['max temperature'].max() # maximum value of the column 'max temperature'\n",
    "\n",
    "if 'binarized max temperature' in FMIRawData.columns:  # delete existing 'binarized max temperature' if there is\n",
    "    FMIRawData = FMIRawData.drop(['binarized max temperature'],axis=1)\n",
    "    \n",
    "bi_labels = [0,1] # new labels to be assigned\n",
    "bi_cut_bins = [minvalue,0,maxvalue] #cutting intervals/criteria [minvalue,0],(0,maxvalue]\n",
    "\n",
    "# Encode max temperatures to binary labels\n",
    "\n",
    "binarized_maxtmp = pd.cut(FMIRawData['max temperature'],\\\n",
    "                                                 bins=bi_cut_bins,labels=bi_labels,include_lowest=True)\n",
    "\n",
    "# Insert a new column \"binarized max temperature\" to the datafame\n",
    "# Int 6 is the position where the new column will be inserted to\n",
    "\n",
    "FMIRawData.insert(6,'binarized max temperature',binarized_maxtmp) \n",
    "\n",
    "# Print the first 5 rows of the DataFrame 'FMIRawData',you will see the new column \"binarized max temperature\"\n",
    "FMIRawData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc835e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1bdc6a2be857ecafadb16a76e72a5d96",
     "grade": false,
     "grade_id": "cell-3481f31ecaf3ec6e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The code snippet below generates two scatter plots and a histogram to visualise the data points in our dataset. The first scatter plot depicts data points (day) using their corresponding min. and max temperature as coordinates. The second scatter plot depicts data points (days) using their min temperature and label $y$ as coordinates. Finally, a histogram is generated to depict the number of data points for each label value.  We can see that the max temperatures of more than 600 days are above zero, only less than 100 days have max temperature below zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0714d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T06:52:17.318035Z",
     "start_time": "2022-01-25T06:52:16.685440Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb6b6b54d23d2deab4c481078a1a75ba",
     "grade": false,
     "grade_id": "cell-e65d7727d3d3f2d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15,4))\n",
    "axes[0].scatter(FMIRawData['min temperature'],FMIRawData['max temperature']);\n",
    "axes[0].set_xlabel(\"mintmp\")\n",
    "axes[0].set_ylabel(\"maxtmp\")\n",
    "axes[0].set_title(\"mintmp vs maxtmp \")\n",
    "\n",
    "axes[1].scatter(FMIRawData['min temperature'],FMIRawData['binarized max temperature']);\n",
    "axes[1].set_xlabel(\"mintmp\")\n",
    "axes[1].set_ylabel(\"binarized_maxtmp\")\n",
    "axes[1].set_yticks([0,1])\n",
    "axes[1].set_title(\"mintmp vs  binarized_maxtmp\")\n",
    "\n",
    "axes[2].hist(FMIRawData['binarized max temperature'])\n",
    "axes[2].set_title('distribution of binarized_maxtmp')\n",
    "axes[2].set_xlabel(\"binary labels\")\n",
    "axes[2].set_ylabel('number of data points')\n",
    "axes[2].set_xticks([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5253a7b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f1aa3992e65a65ba43aeb87806ab593",
     "grade": false,
     "grade_id": "cell-bc49d090c3b0e874",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the following blocks, we will explore the first classification method we have learnt: **Logistic regression**.\n",
    "\n",
    "Logistic regression is a ML method that learns the parameters of a linear map $h(x) = w^{T}x$ that is used to classify data points into a finite number of categories or classes. Each class or category is represented by some label value $y$. For the weather data above, the label value $y=0$ means a cold day ($maxtmp \\leq{0}$) while the label value $y=1$ means a warm day ($maxtmp >0$). \n",
    "\n",
    "There are two basic variants of logistic regression:\n",
    " \n",
    "\n",
    "- Binary logistic regression for data points having binary labels (they belong to one out of two possible classes). Example: Spam or Not\n",
    "\n",
    "- Multinomial Logistic Regression for data points belonging to one out of three or more categories without ordering. Example: Predicting which color is preferred more (Green, Yellow, Pink)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802a0e1b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a7ccbae3c5b84cfbf42f039038cd763",
     "grade": false,
     "grade_id": "cell-1aa5d80bc5876d91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "## Student Task A2.1\n",
    "\n",
    "1. Fit logistic regression model\n",
    "    - scikit-learn's [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class\n",
    "    - Just like linear regression you will\n",
    "        1. Create a classifier object that represents a combination of hypothesis space (linear maps) and loss function (logistic loss)\n",
    "        2. Call the fit method\n",
    "        3. Call the predict method\n",
    "    \n",
    "    \n",
    "2. Calculate Accuracy\n",
    "   \n",
    "    Use the `accuracy` metric to evaluate the model, the function [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) has been imported for you in the beginning of this notebook.\n",
    "    \n",
    "NOTE: the .fit() function uses the (average) logistic loss to measure the quality of a specific linear map $h(x) = w^{T}x$. The accuracy metric instead is based on the average $0/1$ loss which is easier to interpret but results in a more difficult optimization problem (to be solved by .fit()). \n",
    "    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd6bd69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T06:52:17.328880Z",
     "start_time": "2022-01-25T06:52:17.321554Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "955a2d43279b6aee6677928149a11307",
     "grade": false,
     "grade_id": "cell-e49852494f7909bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the feature matrix X, make sure X.shape==(m,1)\n",
    "# and create label vector y from 'binarized max temperature', y.shape=(m,)\n",
    "X = FMIRawData['min temperature'].to_numpy().reshape(-1, 1)\n",
    "y = FMIRawData['binarized max temperature'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c101e77e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T06:52:19.181652Z",
     "start_time": "2022-01-25T06:52:17.334029Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f51e2af7761c33ac66d7689553d7acce",
     "grade": false,
     "grade_id": "cell-4ed5612ecd53936e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## create a LogisticRegressor clf_1 (do not change the variable names!) and fit the model to the data as:\n",
    "\n",
    "# clf_1 = ...    # initialise a LogisticRegression classifier, use default value for all arguments\n",
    "# clf_1...       # fit cfl_1 to data \n",
    "# y_pred = ...   # compute predicted labels for training data\n",
    "# accuracy = ... # compute accuracy on the training set\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(\"accuracy of LogReg : \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dfd640-4eeb-479e-8144-bf0e73c54e10",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93c80c02543ff11f84ab3e26fca22a84",
     "grade": true,
     "grade_id": "cell-0b497d080ed63873",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests, please leave it as it is\n",
    "\n",
    "## sanity check\n",
    "assert isinstance(clf_1, LogisticRegression), \"please initialise a LogisticRegression classifier\"\n",
    "assert np.isclose(clf_1.coef_,0.65461766), \" learnt parameters are incorrect, please check the feature and label\"\n",
    "assert len(y_pred) == len(y), \"the lengthe of y_pred is incorrect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d91d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T06:52:19.656215Z",
     "start_time": "2022-01-25T06:52:19.185665Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b651e842f785c355f0e6ec28ef678ce0",
     "grade": false,
     "grade_id": "cell-b20e29cc7abe0678",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"feature(mintmp)\")\n",
    "ax.set_yticks([0,1])\n",
    "ax.set_ylabel('label (class)')\n",
    "ax.set_title(\"mintmp vs maxtmp (binarized)\")\n",
    "ax.scatter(X[:,0],y,s=50,c=\"skyblue\",label=\"training datapoints\")\n",
    "X_fit = np.linspace(-25, 25, 100) \n",
    "ax.scatter(X_fit, clf_1.predict(X_fit.reshape(-1, 1)),color='r',s=5,label='predicted label ($\\hat{y}=1$ if $h(x) > 0$)') \n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8029c780",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T06:52:19.671091Z",
     "start_time": "2022-01-25T06:52:19.664573Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c5b654df59e19821c69e400c8ddebc5",
     "grade": true,
     "grade_id": "cell-97d357ab5951dc15",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests, please leave it as it is\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8d18fe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fdc81514c7735dc8001d4f8d06e170e",
     "grade": false,
     "grade_id": "cell-ce32e4f7d3a628c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Evaluation Metrics for Classification Models\n",
    "Now, you have successfully built your classification model and evaluated your model with the most commonly used  metric `accuracy`. Accuracy is defined as the ratio of the number of correct predictions made by the model over all kinds of predictions made. Note that the accuracy is given by $1 - (1/m) \\sum_{i=1}^{m} L(.)$ with $L$ being the 0/1 loss (see Sec. 2.3 of [course book](https://github.com/alexjungaalto/MachineLearningTheBasics/blob/master/MLBasicsBook.pdf)).\n",
    "\n",
    "\n",
    "You may have noticed that the dataset used in task A3.1 is imbalanced in the sense of a significantly varying frequency of different categories. In particular, there have been much more data points with $y^{(i)} =1$ compared to data points with $y^{(i)} = 0$. If you created the logistic model correctly, you have reached a decent accuracy of around 96%, however, note that you can easily get training accuracy around 90% by always predicting max temperature being above zero. In practice, the situation could be even more extreme. For example, when you want to build a classifier for a medical diagnoses of a rare but fatal disease with a very low incidence 0.1%, you can get an accuracy 99.9% without any effort (just by always predicting \"no disease\"), but the cost of failing to diagnose the disease of a sick person is dire. This is why we will use different types of evaluation metrics to measure the performance of a hypothesis in classification problems. \n",
    "\n",
    "Scikit-learn provides multiple metrics, such as [precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score), [recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score), [accurcy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) and [f1-score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score). These more fine-grained performance measures can be obtained from the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). Confusion matrix, also known as an error matrix, is a K x K matrix used for evaluating the performance of a classification model, where K is the number of label classes. The matrix compares the true label values with those predicted by the model. The $k$-th row and $l$-th column entry of confusion matrix indicates the number of samples with true label being $k$-th class and predicted label being $l$-th class. It gives a overall view of how well the model is performing and what kinds of errors it is making.\n",
    "\n",
    "This is the confusion matrix for a binary classification model:\n",
    "TN: true negative, FN: false negative, TP: true positive, FP: false positive\n",
    "\n",
    "\n",
    "<p> \n",
    "<img src=\"con_mat.png\" width=\"500\" height=\"350\">\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd0ffb9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2ad1658a0cd1f12fe934db54f69e3ec",
     "grade": false,
     "grade_id": "cell-1aa5d89bc5276d91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "## Student Task A2.2\n",
    "\n",
    "Compute confusion matrix to evaluate the performance of the classification model you've just learnt.\n",
    "The function [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to create confusion matrix is already imported for you.The input for confusion_matrix() should be true labels and predicted labels\n",
    "    \n",
    "    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1488c13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T06:52:19.689090Z",
     "start_time": "2022-01-25T06:52:19.678428Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0ad7863b8ec8f837cfbb4ec7240b9b5",
     "grade": false,
     "grade_id": "cell-3da7cf429b420d0c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## compute confusion matrix as:\n",
    "# conf_mat = confusion_matrix(..., ...) # NOTE: please pay attention to the order of arguments, the order matters.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(conf_mat) #print the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d74b1e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T06:52:19.705958Z",
     "start_time": "2022-01-25T06:52:19.693696Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b7ad7d43b7d8d5ab8ea97cd59c855d6",
     "grade": true,
     "grade_id": "cell-f389f34af3fbe0e8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests, please leave it as it is\n",
    "\n",
    "## sanity check\n",
    "assert conf_mat.shape == (2,2), \"Generated confusion matrix has incorrect shape\"\n",
    "assert conf_mat[0, 1] == 17, \"Generated confusion matrix is incorrect\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3fd7b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T06:52:20.079693Z",
     "start_time": "2022-01-25T06:52:19.713605Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "922d0e8de799885b08bc4fd22857bdcf",
     "grade": false,
     "grade_id": "cell-85951b33c75487d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the confusion matrix you computed\n",
    "ax= plt.subplot()\n",
    "\n",
    "sns.heatmap(conf_mat, annot=True, fmt='g', ax=ax)\n",
    "\n",
    "ax.set_xlabel('Predicted labels',fontsize=15)\n",
    "ax.set_ylabel('True labels',fontsize=15)\n",
    "ax.set_title('Confusion Matrix',fontsize=15)\n",
    "ax.xaxis.set_ticklabels(['below zero', 'above zero'],fontsize=15)\n",
    "ax.yaxis.set_ticklabels(['below zero', 'above zero'],fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a39dd4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd31eff671cdcfb7b0b3009a15804705",
     "grade": false,
     "grade_id": "cell-1aa5d80bc5276d91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "## Student Task A2.3\n",
    "\n",
    "Compute precision from the confusion matrix you have created, consider max temperature above zero as positive.\n",
    "    \n",
    "    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d902e348",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T06:52:20.091064Z",
     "start_time": "2022-01-25T06:52:20.082707Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83ee5ad4a5118d9df439fa87d0f27cc8",
     "grade": false,
     "grade_id": "cell-cf453d2f25858dd8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## compute precision according to the confusion matrix above, and assign it to the variable 'precision':\n",
    "# precision = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('The precision of the model is: ',precision)\n",
    "assert isinstance(precision, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834680e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T06:52:20.108596Z",
     "start_time": "2022-01-25T06:52:20.101895Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a9abc18ef2a917d7a2576e94744c69e",
     "grade": true,
     "grade_id": "cell-59f654adaf6f9d0e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests, please leave it as it is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3730616",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c76d737110679b8b25c41384fa3cc73",
     "grade": false,
     "grade_id": "cell-29f299f7457b6d9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "## Student Task A2.4\n",
    "1. Categorize 'max temperature' into 4 categories by using the same [pd.cut()](https://pandas.pydata.org/docs/reference/api/pandas.cut.html) method which is used to binarize 'max temperature' before:\n",
    "    - 0: $max\\_temperature \\leq{0}$\n",
    "    - 1: $0<max\\_temperature \\leq{5}$\n",
    "    - 2: $5<max\\_temperature \\leq{10}$\n",
    "    - 3: $max\\_temperature>10$\n",
    "2. Fit a multi classification logistic model to predict the label with the same feature `min temperature` as above.\n",
    "   \n",
    "    In scikit-learn, binary classification model and multi classification model are combined together, so in fact you will call the same class object [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "    \n",
    "3. Calculate Accuracy\n",
    "    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f008ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T06:52:20.162984Z",
     "start_time": "2022-01-25T06:52:20.115152Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0967ad27bd293554f12a9a001ac92b0c",
     "grade": false,
     "grade_id": "cell-37e6eb07458ec3d9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'categorized max temperature' in FMIRawData.columns:  # delete existing 'categorized max temperature' if there is\n",
    "    FMIRawData = FMIRawData.drop(['categorized max temperature'],axis=1)\n",
    "        \n",
    "## Categorize 'max temperature' and add a new column \"categorized max temperature\" to the dataframe:\n",
    "\n",
    "# multi_labels = [0, ..., ..., ...]\n",
    "# multi_cut_bins = [minvalue, ..., ..., ..., maxvalue] #cutting intervals/criteria \n",
    "\n",
    "# categorized_maxtmp = pd.cut(FMIRawData['max temperature'],\\\n",
    "#                                         bins=multi_cut_bins,labels=multi_labels,include_lowest=True)\n",
    "# FMIRawData.insert(7,'categorized max temperature',categorized_maxtmp)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "## print the first 5 rows to see if new column is created correctly\n",
    "FMIRawData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ca1c2a-e7b1-457e-9b8f-7464b048a17e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8a6cc3fd08b6fd0ea62398c456c124a",
     "grade": true,
     "grade_id": "cell-855f23b2419eb0f1",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests, please leave it as it is\n",
    "\n",
    "## sanity check\n",
    "assert multi_cut_bins[2] == 5, \"incorrect cut bins\"\n",
    "assert len(FMIRawData['categorized max temperature'].unique()) == 4,  \"number of categories is incorrect\"\n",
    "assert FMIRawData.loc[0]['categorized max temperature'] == 1, \"label value error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da3454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T06:52:20.440987Z",
     "start_time": "2022-01-25T06:52:20.167448Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebc56ecc10d47f5e0737813cc34cba3a",
     "grade": false,
     "grade_id": "cell-bcf11a2264dd06f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize the distribution of categorized max temperature\n",
    "# visualize mintmp vs categorized maxtmp\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "axes[0].hist(FMIRawData['categorized max temperature']) \n",
    "axes[0].set_xticks([0,1,2,3])\n",
    "axes[0].set_xlabel(\"categories (classes)\")\n",
    "axes[0].set_ylabel(\"number of datapoints\")\n",
    "axes[0].set_title(\"Distribution of categorized max temperatures\")\n",
    "\n",
    "axes[1].scatter(FMIRawData['min temperature'],FMIRawData['categorized max temperature']);\n",
    "axes[1].set_xlabel(\"mintmp\")\n",
    "axes[1].set_ylabel(\"categorized_maxtmp\")\n",
    "axes[1].set_yticks([0,1,2,3])\n",
    "axes[1].set_title(\"mintmp vs  categorized_maxtmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0aa0f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T06:52:20.460990Z",
     "start_time": "2022-01-25T06:52:20.452284Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6408d2310637b3f148107392c6a370d2",
     "grade": false,
     "grade_id": "cell-e7cbe68e8c124eb6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## create the feature matrix X_multi from the column \"min temperature\", make sure X_multi.shape==(m,1)\n",
    "## create label vector y_multi from 'categorized max temperature' column, y_multi.shape==(m,)\n",
    "\n",
    "# X_multi = ...\n",
    "# y_multi = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078559e-7619-405c-93cd-2813cd61886b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fc9533897d3d7d56739e934956c9c6d",
     "grade": true,
     "grade_id": "cell-233b919cd4886a5a",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests, please leave it as it is\n",
    "\n",
    "#sanity check the shape of the feature matrix and the label vector\n",
    "assert X_multi.shape == (FMIRawData.shape[0],1), \"feature matrix shape error\"\n",
    "assert y_multi.shape == (FMIRawData.shape[0],),\"label vector shape error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1478ad6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T06:52:20.509161Z",
     "start_time": "2022-01-25T06:52:20.466915Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38c345352f445de8887a46cca42a21df",
     "grade": false,
     "grade_id": "cell-b7687446b9cf04f5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## create a LogisticRegressor and train it\n",
    "## NOTE: please carefully check variable names, DO NOT overwrite variable names used in A3.1\n",
    "\n",
    "# clf_2 = ...           # initialise a LogisticRegression classifier, use default value for all arguments\n",
    "# clf_2...              # fit the model to the data  \n",
    "# y_multi_pred = ...    # compute predicted labels for training datapoints\n",
    "# multi_accuracy = ...  #compute accuracy on the training set\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"accuracy of multi classification : \", multi_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebc0e8a-63fe-4910-ad31-fab1ce024859",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa14b73905c326bc9825bbf2cda1fec9",
     "grade": true,
     "grade_id": "cell-e1ae5b44b5f2dad1",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests, please leave it as it is\n",
    "\n",
    "# sanity check\n",
    "assert isinstance(clf_2, LogisticRegression), \"please use LogisticRegression model\"\n",
    "assert np.isclose(clf_2.coef_[0],-0.79953543), \"learnt parameters are incorrect, please check the feature and label\"\n",
    "assert y_multi_pred[0] == 1, \"prediction error, please check the feature\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f6892",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T06:52:20.784733Z",
     "start_time": "2022-01-25T06:52:20.522032Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7b964cc20839b80f2375405b79191d6",
     "grade": false,
     "grade_id": "cell-11e60076fc94e4e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "# when plotting a single plot, we can omit creating a figure object\n",
    "plt.scatter(X_multi[:,0],y_multi,s=50,c=\"skyblue\",label=\"training datapoints\")\n",
    "\n",
    "X_fit = np.linspace(-25, 25, 100) \n",
    "plt.scatter(X_fit, clf_2.predict(X_fit.reshape(-1, 1)),color='red',s=5,label=\"predicted label ($\\hat{y}$)\") \n",
    "\n",
    "plt.xlabel(\"feature (mintemp)\")\n",
    "plt.ylabel(\"label /category /class\")\n",
    "plt.yticks([0,1,2,3])\n",
    "plt.title(\"min tmp vs categorized max temp\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade10ea2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T06:52:20.803620Z",
     "start_time": "2022-01-25T06:52:20.791434Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ad944afd1475b8ab29d32ff567fc962",
     "grade": true,
     "grade_id": "cell-f389f34af2fbe0e8",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6aa986",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a83e8883f97b49a22fdf305b61a36170",
     "grade": false,
     "grade_id": "cell-48e0cb63a2cf27bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Visualization\n",
    "\n",
    "We'll go over some ways in which one may lower the amount of features used for training even without domain expertise, i.e., without being able to select them manually based on one's understanding of the relationships between the features and the label. There are multiple reasons one might want to use less features in training: \n",
    "* to prevent overfitting (improving the ratio of training samples to features)\n",
    "* to decrease computational complexity (faster training and prediction)\n",
    "* visualizing high-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33980a16",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbcb6b286c2a41decf6821fc309267cd",
     "grade": false,
     "grade_id": "cell-e7f868ee1dcc94ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "You can think of Principal component analysis as finding a linear subspace of a given dimension (line, plane, hyperplane,...) that is the best fit of high-dimensional vectors. In this case, by best fit we mean that we minimize the squares of the distances of the vectors from the subspace, which corresponds to the information that we'll lose by this method.\n",
    "\n",
    "If you recall how linear regression works, this may sound familiar. The difference is that linear regression studies functional dependence of the label on the features (how to fit a linear predictor), while PCA does not take the labels into account, it is only concerned about the features. The exact formulations are out of scope of this assignment but for visualizations, see for example [this link](https://www.r-bloggers.com/2010/09/principal-component-analysis-pca-vs-ordinary-least-squares-ols-a-visual-explanation/). \n",
    "\n",
    "There are a few pitfalls to PCA that you should be aware of:\n",
    "* If we set the dimensions of the subspace too low, we may lose too much information to make any reasonable predictions.\n",
    "* The features generated by PCA are a linear combination of the original features and there's likely no way to interpret them. Imagine a dataset where you try to predict car brand based on engine power and price. Using PCA to reduce the features to 1 might leave us with a feature \"0.9\\*price-0.1\\*power\", which doesn't have any real-world meaning. \n",
    "* Since PCA doesn't take labels into account, it is entirely possible that the selected features will not be optimal for predicting the labels.\n",
    "* PCA is very sensitive to statistical properties of the different dimensions (=features).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842bcb2e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "054003e44f179909acd9804b84b1936b",
     "grade": false,
     "grade_id": "cell-8bc1eeae224b1c00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### MNIST\n",
    "We begin by fetching the MNIST dataset. This dataset contains 70000 B&W images with resolution 28x28 pixels. Each pixel represents a shade of grey as an 8bit integer (range 0-255) and each image represents a handwritten digit. This leads to a 10-class classification problem, where the goal is to predict the digit based on pixel intensities.\n",
    "\n",
    "Below we fetch the datset and view one representative of each class to familiarize ourselves with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9350b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee53e46c4ada3e1623598b6681ef87b7",
     "grade": false,
     "grade_id": "cell-36bc52ee041a037a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    X_MNIST, y_MNIST = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False, data_home=\"/coursedata\", cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d75e9b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3042a754cf254e86a19c9faffbb72d65",
     "grade": false,
     "grade_id": "cell-d77f4a78736579d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the numbers in two rows\n",
    "def plot_digits(X,y,cols=5,title=None):\n",
    "    cols = 5\n",
    "    fig, axs = plt.subplots(2, cols)\n",
    "    plt.axis('off')\n",
    "    for digit in range(10):\n",
    "        # find the first representative of the label\n",
    "        idx = np.argwhere(y == str(digit))[0]\n",
    "\n",
    "        # change the vector into a 2D array and plot it\n",
    "        im = X[idx].reshape(28, 28)\n",
    "        axs[digit//cols, digit % cols].set_title(f'Class {digit}')\n",
    "        axs[digit//cols, digit % cols].imshow(im, cmap='gray')\n",
    "        axs[digit//cols, digit % cols].axis('off')\n",
    "\n",
    "    # change the spacing between the subplots\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=-0.3)\n",
    "    if title is not None:\n",
    "        fig.suptitle(title,fontsize=14)\n",
    "        fig.subplots_adjust(top=0.97)\n",
    "    \n",
    "plot_digits(X_MNIST,y_MNIST,title=\"Examples of each class in the MNIST dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b171ec87",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ced77365c016dfe9987fc57d765ac0ab",
     "grade": false,
     "grade_id": "cell-edee25286e0afd31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_MNIST, y_MNIST, test_size=0.33, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(solver='liblinear' )\n",
    "clf.fit(X_train[:200], y_train[:200])\n",
    "# can increase training data to reach 91.7 % but it takes 10 minutes on my computer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c269fbf3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83b4366da1e6dc33df4947c01abac3b4",
     "grade": false,
     "grade_id": "cell-9a89959a44938605",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_confusion_matrix(y_true, y_pred):\n",
    "    # visualize the confusion matrix\n",
    "    ax = plt.subplot()\n",
    "    c_mat = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(c_mat, annot=True, fmt='g', ax=ax)\n",
    "\n",
    "    ax.set_xlabel('Predicted labels', fontsize=15)\n",
    "    ax.set_ylabel('True labels', fontsize=15)\n",
    "    ax.set_title('Confusion Matrix', fontsize=15)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "generate_confusion_matrix(y_val, y_pred)\n",
    "plt.show()\n",
    "\n",
    "# compute the accuracy\n",
    "multi_accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Prediction accuracy: {100*multi_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505d8014",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e1e41cfc6053c9588fbc3f751b88401",
     "grade": false,
     "grade_id": "cell-d4f4ee5a1f41aaf3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "####  PCA Features\n",
    "\n",
    "First, we fit the PCA to the training data. By setting the ``n_components`` parameter, we can choose the dimension of the fitted subspace (=number of output features). A perhaps surprising result is that having computed the PCA, it is very simple to lower the amount of features further: the most important feature (in the PCA sense) is the first one, the most important two are the first two,... \n",
    "\n",
    "One way to decide how many components to use is to look at the explained variance ratio of each of the features. These correspond to the \"importance\" of each feature. Let's plot these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999347a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3953a223a7c805a46e2c3953869cd6de",
     "grade": false,
     "grade_id": "cell-b26c2b90cc9b9f65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit the PCA\n",
    "N = 50\n",
    "pca = PCA(n_components=N)\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "# plot the explained variances\n",
    "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "color = 'tab:blue'\n",
    "ax1.bar(1+np.arange(N), pca.explained_variance_ratio_, color=color)\n",
    "ax1.set_xticks(1+np.arange(N, step=2))\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_ylabel(\"Explained variance ratio\", color=color)\n",
    "ax1.set_xlabel(\"Generated feature\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.plot(1+np.arange(N), np.cumsum(pca.explained_variance_ratio_), color=color)\n",
    "ax2.set_ylabel(\"Cumulative explained variance ratio\", color=color)\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0356cd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c813e060b305f48268de74b832ba793d",
     "grade": false,
     "grade_id": "cell-2d18fb71fe0444c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If we choose to use 784 components, then the cumulative explained variance ratio will be 1, i.e., no data would be lost. This corresponds to projecting the data to the same space where it originally \"lived\", i.e. not doing anything. Note, however, that the features will likely still change, even though no data is lost. What happens is essentially a change of basis of the feature space. \n",
    "\n",
    "Of course, there's not much use in keeping all the features, so how many features (components) should we keep? There's no clear cut answer to that here and there rarely is. In this case, let's keep 15 components, because the explained variance ratio drops to under 2% per feature around that point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4462777",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63b676e59110f5bffe5e7320011d4a76",
     "grade": false,
     "grade_id": "cell-2104106a87c1df9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 15\n",
    "pca.set_params(n_components=N)\n",
    "X_train_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759fc96",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24e35aaa6e3cfdfd27262c71e703a607",
     "grade": false,
     "grade_id": "cell-dbf57f90eff1fbac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's visually confirm that the compression hasn't affected the pictures too much. Based on the representatives chosen, it seems that keeping just 15/784 dimensions still leaves the digits legible, if slightly blurry.\n",
    "\n",
    "To view the image after dropping the less important features, we need to transform the 15-dimensional vector back into the original 784-dimensional space. For this, we'll use the ``inverse_transform`` method. Mathematically, all of the components form a basis of the original space, so this function creates a linear combination of the first 15 basis vector with coefficients corresponding to the new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168db35",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e814c9649f03fbc8db2420595679298",
     "grade": false,
     "grade_id": "cell-1455c31985b4c38f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the digits with only 15 components\n",
    "plot_digits(pca.inverse_transform(X_train_reduced), y_train, title=\"An example of each class with 15 PCA components\")\n",
    "plt.show()\n",
    "\n",
    "# plot the same picture with different numebrs of features\n",
    "digit = 4\n",
    "cols = 8\n",
    "fig, axs = plt.subplots(2, cols, figsize=(12, 8))\n",
    "fig.suptitle(\"An image of a '4' with varying number of PCA components\", fontsize=14)\n",
    "idx = np.argwhere(y_train == str(digit))[0]\n",
    "for n in 1+np.arange(N):\n",
    "    X_nfeatures = X_train_reduced[idx]*[1 if i < n else 0 for i in range(N)]\n",
    "    # transform the data back into the original 784-dimensional space\n",
    "    im = pca.inverse_transform(X_nfeatures).reshape(28, 28)\n",
    "    # plot it\n",
    "    axs[(n-1)//cols, (n-1) % cols].set_title(f\"{(n)} feature{'s' if n>1 else ''}\")\n",
    "    axs[(n-1)//cols, (n-1) % cols].imshow(im, cmap='gray')\n",
    "    axs[(n-1)//cols, (n-1) % cols].axis('off')\n",
    "# plot the original\n",
    "axs[-1, -1].set_title(f'original')\n",
    "axs[-1, -1].imshow(X_train[idx].reshape(28, 28), cmap='gray')\n",
    "axs[-1, -1].axis('off')\n",
    "# adjust spacing\n",
    "fig.subplots_adjust(wspace=0.1, hspace=-0.8, top=1.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32078f8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "392ff56059354dddfa1471de038ed187",
     "grade": false,
     "grade_id": "cell-b05768f6c90a9633",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "## Student Task A2.5\n",
    "Use the transformed data ``X_train_reduced`` to train a logistic regressor. Then compute its accuracy on the validation set. Since the logistic regressor will be trained with PCA transformed data, for doing predictions one has to first transform the validation set using the PCA transformation learned on the training set. That can be done using the transform method of the fitted PCA instance (see https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.transform).\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f8d64",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d92b835cdcb533df33dfc4a7c99992e0",
     "grade": false,
     "grade_id": "cell-c0ae033043155e9d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clf_2 = # create the object, SET solver='sag'\n",
    "# clf_2. # fit the data\n",
    "# y_pred = # compute the prediction on the validating set\n",
    "# multi_accuracy = # compute the accuracy score\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(f\"Prediction accuracy: {100*multi_accuracy:.2f}%\")\n",
    "generate_confusion_matrix(y_val, y_pred)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122a017",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "992de468cdaa5e238a4e332e5d006fd5",
     "grade": true,
     "grade_id": "cell-1626da5936462cbd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## test cell for A2.5\n",
    "\n",
    "# sanity check\n",
    "assert isinstance(clf_2, LogisticRegression), \"You need to use Logistic regression!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5391b2a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87346d864ffbd5f493fb790ab6a9ee25",
     "grade": true,
     "grade_id": "cell-f0e0b000d9670f9e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## test cell for A2.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31340356",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af90e087e26ecba45bf36064d947d69a",
     "grade": false,
     "grade_id": "cell-0c69fbdfc3e38ecf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Demo: Data visualization\n",
    "Another application of PCA is visualization of high-dimensional data. Here, we plot each image as a single point based on its coordinates in the 2D subspace generated by PCA. For example, we can see that 1's seem to be very easy to tell apart and they all look very similar, whereas these two dimensions are certainly not enough to tell apart 4's and 9's in any meaningful way. This confirms what we saw earlier - with only a few components, the 4 looked like a 9!\n",
    "\n",
    "Note that since the features carry no meaning anymore, we label the axes simply as PCA1 and PCA2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e9db7e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d150bbdae95783bc95ef68bcc4d969a2",
     "grade": false,
     "grade_id": "cell-ce6f9e971f7a8470",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "# convert the labels to numbers, each will be assigned a separate color based on the cmap specified\n",
    "colors = [int(x) for x in y_train]\n",
    "sc = plt.scatter(X_train_reduced[:, 0], X_train_reduced[:, 1], s=1, c=colors, cmap='tab10')\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.legend(*sc.legend_elements(), title='digit')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070108cd-a341-4da5-8f15-10d20d361fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
